{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a0af78",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70950e",
   "metadata": {},
   "source": [
    "reference\n",
    "\n",
    "https://d3rlpy.readthedocs.io/en/stable/tutorials/create_your_dataset.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99718bd1",
   "metadata": {},
   "source": [
    "## Prepare Logged Data\n",
    "\n",
    "*First of all, you need to prepare your logged data. In this tutorial, let’s use randomly generated data. terminals represents the last step of episodes. If terminals[i] == 1.0, i-th step is the terminal state. Otherwise you need to set zeros for non-terminal states.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b32fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# vector observation\n",
    "# 1000 steps of observations with shape of (100,)\n",
    "observations = np.random.random((1000, 100))\n",
    "\n",
    "# 1000 steps of actions with shape of (4,)\n",
    "actions = np.random.random((1000, 4))\n",
    "\n",
    "# 1000 steps of rewards\n",
    "rewards = np.random.random(1000)\n",
    "\n",
    "# 1000 steps of terminal flags\n",
    "terminals = np.random.randint(2, size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcd3ed2",
   "metadata": {},
   "source": [
    "## Build MDPDataset with logged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f773177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-09-15 22:56.07\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mSignatures have been automatically determined.\u001b[0m \u001b[36maction_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float64')], shape=[(4,)])\u001b[0m \u001b[36mobservation_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float64')], shape=[(100,)])\u001b[0m \u001b[36mreward_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float64')], shape=[(1,)])\u001b[0m\n",
      "\u001b[2m2025-09-15 22:56.07\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction-space has been automatically determined.\u001b[0m \u001b[36maction_space\u001b[0m=\u001b[35m<ActionSpace.CONTINUOUS: 1>\u001b[0m\n",
      "\u001b[2m2025-09-15 22:56.07\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction size has been automatically determined.\u001b[0m \u001b[36maction_size\u001b[0m=\u001b[35m4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import d3rlpy\n",
    "import gymnasium as gym\n",
    "dataset = d3rlpy.dataset.MDPDataset(\n",
    "    observations=observations,\n",
    "    actions=actions,\n",
    "    rewards=rewards,\n",
    "    terminals=terminals,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c3dfc9",
   "metadata": {},
   "source": [
    "## Set timeout flags \n",
    "\n",
    "*In RL, there is the case where you want to stop an episode without a terminal state. For example, if you’re collecting data of a 4-legged robot walking forward, the walking task basically never ends as long as the robot keeps walking while the logged episode must stop somewhere. In this case, you can use timeouts to represent this timeout states.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ffd353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-09-15 22:57.02\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mSignatures have been automatically determined.\u001b[0m \u001b[36maction_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float64')], shape=[(4,)])\u001b[0m \u001b[36mobservation_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float64')], shape=[(100,)])\u001b[0m \u001b[36mreward_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float64')], shape=[(1,)])\u001b[0m\n",
      "\u001b[2m2025-09-15 22:57.02\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction-space has been automatically determined.\u001b[0m \u001b[36maction_space\u001b[0m=\u001b[35m<ActionSpace.CONTINUOUS: 1>\u001b[0m\n",
      "\u001b[2m2025-09-15 22:57.02\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction size has been automatically determined.\u001b[0m \u001b[36maction_size\u001b[0m=\u001b[35m4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# terminal states\n",
    "terminals = np.zeros(1000)\n",
    "\n",
    "# timeout states\n",
    "timeouts = np.random.randint(2, size=1000)\n",
    "\n",
    "dataset = d3rlpy.dataset.MDPDataset(\n",
    "    observations=observations,\n",
    "    actions=actions,\n",
    "    rewards=rewards,\n",
    "    terminals=terminals,\n",
    "    timeouts=timeouts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11260302",
   "metadata": {},
   "source": [
    "# Preprocess / Postprocess\n",
    "\n",
    "ref: https://d3rlpy.readthedocs.io/en/stable/tutorials/preprocess_and_postprocess.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3944eeb",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocess Observations\n",
    "\n",
    "*If your dataset includes unnormalized observations, you can normalize or standardize the observations by specifying observation_scaler argument. In this case, the statistics of the dataset will be computed at the beginning of offline training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "250486fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donwloading pendulum.pkl into d3rlpy_data/pendulum_random_v1.1.0.h5...\n",
      "\u001b[2m2025-09-15 22:57.47\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mSignatures have been automatically determined.\u001b[0m \u001b[36maction_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float32')], shape=[(1,)])\u001b[0m \u001b[36mobservation_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float32')], shape=[(3,)])\u001b[0m \u001b[36mreward_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float32')], shape=[(1,)])\u001b[0m\n",
      "\u001b[2m2025-09-15 22:57.47\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction-space has been automatically determined.\u001b[0m \u001b[36maction_space\u001b[0m=\u001b[35m<ActionSpace.CONTINUOUS: 1>\u001b[0m\n",
      "\u001b[2m2025-09-15 22:57.47\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction size has been automatically determined.\u001b[0m \u001b[36maction_size\u001b[0m=\u001b[35m1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import d3rlpy\n",
    "\n",
    "dataset, _ = d3rlpy.datasets.get_dataset(\"pendulum-random\")\n",
    "\n",
    "# prepare scaler without initialization\n",
    "observation_scaler = d3rlpy.preprocessing.StandardObservationScaler()\n",
    "\n",
    "sac = d3rlpy.algos.SACConfig(observation_scaler=observation_scaler).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75771ac6",
   "metadata": {},
   "source": [
    "*Alternatively, you can manually instantiate preprocessing parameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14352bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup manually\n",
    "observations = []\n",
    "for episode in dataset.episodes:\n",
    "    observations += episode.observations.tolist()\n",
    "mean = np.mean(observations, axis=0)\n",
    "std = np.std(observations, axis=0)\n",
    "observation_scaler = d3rlpy.preprocessing.StandardObservationScaler(mean=mean, std=std)\n",
    "\n",
    "# set as observation_scaler\n",
    "sac = d3rlpy.algos.SACConfig(observation_scaler=observation_scaler).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e96f7d",
   "metadata": {},
   "source": [
    "## Preprocess / Postprocess Actions\n",
    "\n",
    "*In training with continuous action-space, the actions must be in the range between [-1.0, 1.0] due to the underlying tanh activation at the policy functions. In d3rlpy, you can easily normalize inputs and denormalize outpus instead of normalizing datasets by yourself.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b7a059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare scaler without initialization\n",
    "action_scaler = d3rlpy.preprocessing.MinMaxActionScaler()\n",
    "\n",
    "# set as action scaler\n",
    "sac = d3rlpy.algos.SACConfig(action_scaler=action_scaler).create()\n",
    "\n",
    "# setup manually\n",
    "actions = []\n",
    "for episode in dataset.episodes:\n",
    "    actions += episode.actions.tolist()\n",
    "minimum_action = np.min(actions, axis=0)\n",
    "maximum_action = np.max(actions, axis=0)\n",
    "action_scaler = d3rlpy.preprocessing.MinMaxActionScaler(\n",
    "    minimum=minimum_action,\n",
    "    maximum=maximum_action,\n",
    ")\n",
    "\n",
    "# set as action scaler\n",
    "sac = d3rlpy.algos.SACConfig(action_scaler=action_scaler).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9fd24",
   "metadata": {},
   "source": [
    "## Preprocess Rewards\n",
    "\n",
    "*The effect of scaling rewards is not well studied yet in RL community, however, it’s confirmed that the reward scale affects training performance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ffe13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.preprocessing import StandardRewardScaler\n",
    "\n",
    "# prepare scaler without initialization\n",
    "reward_scaler = d3rlpy.preprocessing.StandardRewardScaler()\n",
    "\n",
    "# set as reward scaler\n",
    "sac = d3rlpy.algos.SACConfig(reward_scaler=reward_scaler).create()\n",
    "\n",
    "# setup manuall\n",
    "rewards = []\n",
    "for episode in dataset.episodes:\n",
    "    rewards += episode.rewards.tolist()\n",
    "mean = np.mean(rewards)\n",
    "std = np.std(rewards)\n",
    "reward_scaler = StandardRewardScaler(mean=mean, std=std)\n",
    "\n",
    "# set as reward scaler\n",
    "sac = d3rlpy.algos.SACConfig(reward_scaler=reward_scaler).create()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
