{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee30a48",
   "metadata": {},
   "source": [
    "ref: https://d3rlpy.readthedocs.io/en/stable/tutorials/offline_policy_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c5bf4d",
   "metadata": {},
   "source": [
    "# Offline Policy Selection\n",
    "\n",
    "d3rlpy supports offline policy selection by training Fitted Q Evaluation (FQE), which is an offline on-policy RL algorithm. The use of FQE for offline policy selection is proposed by Paine et al.. The concept is that FQE trains Q-function with the trained policy in on-policy manner so that the learned Q-function reflects the expected return of the trained policy. By using the Q-value estimation of FQE, the candidate trained policies can be ranked only with offline dataset. Check Off-Policy Evaluation for more information.\n",
    "\n",
    ">Offline policy selection with FQE is confirmed that it usually works out with discrete action-space policies. However, it seems require some hyperparameter tuning for ranking continuous action-space policies. The more techniques will be supported along with the advancement of this research domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d6730",
   "metadata": {},
   "source": [
    "## Prepare trained policies\n",
    "\n",
    "In this tutorial, let’s train DQN with the built-in CartPole-v0 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07d81ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-09-15 23:21.38\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mSignatures have been automatically determined.\u001b[0m \u001b[36maction_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('int32')], shape=[(1,)])\u001b[0m \u001b[36mobservation_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float32')], shape=[(4,)])\u001b[0m \u001b[36mreward_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float32')], shape=[(1,)])\u001b[0m\n",
      "\u001b[2m2025-09-15 23:21.38\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction-space has been automatically determined.\u001b[0m \u001b[36maction_space\u001b[0m=\u001b[35m<ActionSpace.DISCRETE: 2>\u001b[0m\n",
      "\u001b[2m2025-09-15 23:21.38\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction size has been automatically determined.\u001b[0m \u001b[36maction_size\u001b[0m=\u001b[35m2\u001b[0m\n",
      "\u001b[2m2025-09-15 23:21.39\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mdataset info                  \u001b[0m \u001b[36mdataset_info\u001b[0m=\u001b[35mDatasetInfo(observation_signature=Signature(dtype=[dtype('float32')], shape=[(4,)]), action_signature=Signature(dtype=[dtype('int32')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('float32')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=2)\u001b[0m\n",
      "\u001b[2m2025-09-15 23:21.39\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mBuilding models...            \u001b[0m\n",
      "\u001b[2m2025-09-15 23:21.39\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mModels have been built.       \u001b[0m\n",
      "\u001b[2m2025-09-15 23:21.39\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDirectory is created at d3rlpy_logs/DQN_20250915232139\u001b[0m\n",
      "\u001b[2m2025-09-15 23:21.39\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mParameters                    \u001b[0m \u001b[36mparams\u001b[0m=\u001b[35m{'observation_shape': [4], 'action_size': 2, 'config': {'type': 'dqn', 'params': {'batch_size': 32, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'none', 'params': {}}, 'compile_graph': False, 'learning_rate': 6.25e-05, 'optim_factory': {'type': 'adam', 'params': {'clip_grad_norm': None, 'lr_scheduler_factory': {'type': 'none', 'params': {}}, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'n_critics': 1, 'target_update_interval': 8000}}}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 10000/10000 [00:09<00:00, 1089.14it/s, loss=0.00657]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgymnasium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgym\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# start offline training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mdqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m   \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m   \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m   \u001b[49m\u001b[43mn_steps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m   \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m       \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43menvironment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md3rlpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEnvironmentEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m   \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/python313/lib/python3.13/site-packages/d3rlpy/algos/qlearning/base.py:417\u001b[39m, in \u001b[36mQLearningAlgoBase.fit\u001b[39m\u001b[34m(self, dataset, n_steps, n_steps_per_epoch, experiment_name, with_timestamp, logging_steps, logging_strategy, logger_adapter, show_progress, save_interval, evaluators, callback, epoch_callback)\u001b[39m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\n\u001b[32m    371\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    372\u001b[39m     dataset: ReplayBufferBase,\n\u001b[32m   (...)\u001b[39m\u001b[32m    384\u001b[39m     epoch_callback: Optional[Callable[[Self, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    385\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]]:\n\u001b[32m    386\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Trains with given dataset.\u001b[39;00m\n\u001b[32m    387\u001b[39m \n\u001b[32m    388\u001b[39m \u001b[33;03m    .. code-block:: python\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    415\u001b[39m \u001b[33;03m        List of result tuples (epoch, metrics) per epoch.\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     results = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfitter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_steps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwith_timestamp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_timestamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlogging_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlogger_adapter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_adapter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m            \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m            \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m            \u001b[49m\u001b[43mepoch_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/python313/lib/python3.13/site-packages/d3rlpy/algos/qlearning/base.py:577\u001b[39m, in \u001b[36mQLearningAlgoBase.fitter\u001b[39m\u001b[34m(self, dataset, n_steps, n_steps_per_epoch, logging_steps, logging_strategy, experiment_name, with_timestamp, logger_adapter, show_progress, save_interval, evaluators, callback, epoch_callback)\u001b[39m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m evaluators:\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, evaluator \u001b[38;5;129;01min\u001b[39;00m evaluators.items():\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m         test_score = \u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    578\u001b[39m         logger.add_metric(name, test_score)\n\u001b[32m    580\u001b[39m \u001b[38;5;66;03m# save metrics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/python313/lib/python3.13/site-packages/d3rlpy/metrics/evaluators.py:543\u001b[39m, in \u001b[36mEnvironmentEvaluator.__call__\u001b[39m\u001b[34m(self, algo, dataset)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    541\u001b[39m     \u001b[38;5;28mself\u001b[39m, algo: QLearningAlgoProtocol, dataset: ReplayBufferBase\n\u001b[32m    542\u001b[39m ) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevaluate_qlearning_with_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m=\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_n_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_epsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/python313/lib/python3.13/site-packages/d3rlpy/metrics/utility.py:65\u001b[39m, in \u001b[36mevaluate_qlearning_with_environment\u001b[39m\u001b[34m(algo, env, n_trials, epsilon)\u001b[39m\n\u001b[32m     60\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     61\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported observation type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(observation)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     62\u001b[39m         )\n\u001b[32m     63\u001b[39m     action = algo.predict(observation)[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m observation, reward, done, truncated, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m episode_reward += \u001b[38;5;28mfloat\u001b[39m(reward)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/python313/lib/python3.13/site-packages/gym/wrappers/time_limit.py:50\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[32m     40\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m     41\u001b[39m \n\u001b[32m     42\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m \n\u001b[32m     49\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/python313/lib/python3.13/site-packages/gym/wrappers/order_enforcing.py:37\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/python313/lib/python3.13/site-packages/gym/wrappers/env_checker.py:37\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.checked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mself\u001b[39m.checked_step = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.step(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/python313/lib/python3.13/site-packages/gym/utils/passive_env_checker.py:233\u001b[39m, in \u001b[36menv_step_passive_checker\u001b[39m\u001b[34m(env, action)\u001b[39m\n\u001b[32m    230\u001b[39m obs, reward, terminated, truncated, info = result\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(terminated, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbool8\u001b[49m)):\n\u001b[32m    234\u001b[39m     logger.warn(\n\u001b[32m    235\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(terminated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    236\u001b[39m     )\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncated, (\u001b[38;5;28mbool\u001b[39m, np.bool8)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/python313/lib/python3.13/site-packages/numpy/__init__.py:414\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchar\u001b[39;00m\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m char.chararray\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    415\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\u001b[34m__name__\u001b[39m, attr))\n",
      "\u001b[31mAttributeError\u001b[39m: module 'numpy' has no attribute 'bool8'"
     ]
    }
   ],
   "source": [
    "import d3rlpy\n",
    "\n",
    "# setup replay CartPole-v0 dataset and environment\n",
    "dataset, env = d3rlpy.datasets.get_dataset(\"cartpole-replay\")\n",
    "\n",
    "# setup algorithm\n",
    "\n",
    "dqn = d3rlpy.algos.DQNConfig().create()\n",
    "\n",
    "import gymnasium as gym\n",
    "# start offline training\n",
    "dqn.fit(\n",
    "   dataset,\n",
    "   n_steps=100000,\n",
    "   n_steps_per_epoch=10000,\n",
    "   evaluators={\n",
    "       \"environment\": d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "   },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "135667a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-09-15 23:23.28\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mSignatures have been automatically determined.\u001b[0m \u001b[36maction_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('int32')], shape=[(1,)])\u001b[0m \u001b[36mobservation_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float32')], shape=[(4,)])\u001b[0m \u001b[36mreward_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float32')], shape=[(1,)])\u001b[0m\n",
      "\u001b[2m2025-09-15 23:23.28\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction-space has been automatically determined.\u001b[0m \u001b[36maction_space\u001b[0m=\u001b[35m<ActionSpace.DISCRETE: 2>\u001b[0m\n",
      "\u001b[2m2025-09-15 23:23.28\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction size has been automatically determined.\u001b[0m \u001b[36maction_size\u001b[0m=\u001b[35m2\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'd3rlpy.ope' has no attribute 'DiscreteFQEConfig'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m dqn = d3rlpy.load_learnable(\u001b[33m\"\u001b[39m\u001b[33md3rlpy_logs/DQN_20250915221802/model_10000.d3\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# setup FQE algorithm\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m fqe = d3rlpy.ope.DiscreteFQE(algo=dqn, config=\u001b[43md3rlpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mope\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDiscreteFQEConfig\u001b[49m())\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# start FQE training\u001b[39;00m\n\u001b[32m     13\u001b[39m fqe.fit(\n\u001b[32m     14\u001b[39m    dataset,\n\u001b[32m     15\u001b[39m    n_steps=\u001b[32m10000\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m    },\n\u001b[32m     21\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: module 'd3rlpy.ope' has no attribute 'DiscreteFQEConfig'"
     ]
    }
   ],
   "source": [
    "import d3rlpy\n",
    "\n",
    "# setup the same dataset used in policy training\n",
    "dataset, _ = d3rlpy.datasets.get_dataset(\"cartpole-replay\")\n",
    "\n",
    "# load pretrained policy\n",
    "dqn = d3rlpy.load_learnable(\"d3rlpy_logs/DQN_20250915221802/model_10000.d3\")\n",
    "\n",
    "# setup FQE algorithm\n",
    "fqe = d3rlpy.ope.DiscreteFQE(algo=dqn, config=d3rlpy.ope.DiscreteFQEConfig())\n",
    "\n",
    "# start FQE training\n",
    "fqe.fit(\n",
    "   dataset,\n",
    "   n_steps=10000,\n",
    "   n_steps_per_epoch=1000,\n",
    "   scorers={\n",
    "       \"init_value\": d3rlpy.metrics.InitialStateValueEstimationEvaluator(),\n",
    "       \"soft_opc\": d3rlpy.metrics.SoftOPCEvaluator(180),  # set 180 for success return threshold here\n",
    "   },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0730882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ddd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ebe742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
